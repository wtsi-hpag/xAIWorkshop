{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning From First Principles\n",
    "### A Workshop by Jack Fraser-Govil\n",
    "\n",
    "\n",
    "Hello, and welcome to this notebook. Along with the notes, presentation and discussions which we will be having throughout the day, this document will walk you through a series of tasks and exercises which will culminate, by the end of the workshop, in you possessing a fully working Feedforward Neural Network which you have designed and written from scratch. \n",
    "\n",
    "In doing so, we hope that you will gain a degree of insight into how these machines are `thinking' under the hood, and thereby pull back some of the mystery and folklore that surrounds ML, and thereby gain a peek inside the black box. We hope that this knowledge will help inform the discussions and presentations for the rest of this conference. \n",
    "\n",
    "## What Do I Need?\n",
    "\n",
    "In order to run this notebook, all you need is the ability to run an ipython/Jupyter notebooks, and some standard python libraries: mostly ```numpy``` and ```matplotlib```. You do *not* need to have ```tensorflow``` or ```pytorch```: the entire point of this exercise is to create a learning framework without relying on pre-built tools.\n",
    "\n",
    "You *will* need to have read, or be confident that you already know the mathematics contained within [the provided notes](../Notes/Notes.pdf). If you haven't read them yet (and are not confident that your skillset includes the vector-chain rule and matrix algebra), then I suggest you speed-read Chapters 2, 3 and 4.\n",
    "\n",
    "The final thing you need is a willingness to engage with some of the murkier, more difficult and mathematically complex components of the workshop: teasing out what these things *mean* is why you are here; so if something sticks out to you as something you don't understand, don't just let it wash over you -- please ask and we can discuss. \n",
    "\n",
    "\n",
    "## What if I get stuck?\n",
    "\n",
    "The first port of call is always to ask me, one of my helpers, or simply someone near to you, for guidance. \n",
    "\n",
    "If there's a concept you just can't grasp during the day, I will point you to some resources that you can look at afterwards, and hopefully things will become clearer in retrospect. \n",
    "\n",
    "If you get stuck on one of the coding exercises, or cannot complete it before it is time to move on, you will find a [second notebook in the Hints directory](Hints/notebook_solved.ipynb). This notebook will have all of the coding exercises solved for you. That is not to say that it is 'correct', only that this notebook will be *functional*. If you get really, really, really stuck, you can start stealing code from that document. \n",
    "\n",
    "##### However\n",
    "\n",
    "Whilst it is certainly possible (and permitted) for you to do this at any time, the entire purpose of this workshop is for you to come away with the satisfaction of having written your own learning networks from scratch. We are, by definition, reinventing the wheel here: everything we make will be orders of magnitude worse than pytorch or tensorflow; the principle is that **you learn best by doing it yourself**.\n",
    "\n",
    "If you're going to use the 'cheat sheet', then try, first of all, to simply look at it to understand what it is doing, and then implement your own version. Don't directly copy whole chunks of code you don't understand, as that sort of defeats the point of today's efforts.\n",
    "\n",
    "\n",
    "## Jack, Why is this Python Written Like Garbage?\n",
    "\n",
    "That's a very good and very important question. **I am not a python developer**. As a member of the High Performance Algorithms group, I spend most of my time buried in C++ and C code. I have written this workshop assuming that, since the majority of existing ML products are pythonic, that python would be the most accessible language. There are often times where my python code is written like someone used to C-syntax, and blithely unaware to the beautiful, elegant, pythonic way of doing things.\n",
    "\n",
    "I'm afraid you'll just have to deal with my slightly sub-par python. \n",
    "\n",
    "What was **not** a result of subpar python is the choice not to use ```pandas``` for data. I made a conscious decision to keep things as 'mathematical' as possible, rather than abstracted into dataframes. You get a list of vectors (and maybe some labels), and have to deal with them manually, that's part of the bare-bones experience!\n",
    "\n",
    "I also use ```CamelCase```, not ```snake_case```. Fight me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: The Pretrained Perceptron\n",
    "\n",
    "Just to get the juices flowing, let's get started with an easy project.\n",
    "\n",
    "**In the ```Data``` directory is a file ```cuteness.tex```, which contains (non-dimensional) measures of size and furriness of some animals, as well as some human-curated labels (1 for cute, 0 for note cute). Please write a Perceptron function to evaluate (without the labelled data) which of these creatures the model currently mislabels.**  \n",
    "\n",
    "I've provided some basic functions and a class framework for you to work from.\n",
    "\n",
    "Recall that for an input vector $\\mathbf{x}$ and weights $\\mathbf{w}$, the perception algorithm is:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\mathbf{x}) = \\begin{cases} 1 & \\text{if } \\mathbf{w}\\cdot \\begin{bmatrix} 1 \\\\ \\mathbf{x} \\end{bmatrix} > 0 \\\\ 0 & \\text{else}\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are running this in Google Colabs, then you need to uncomment these lines; they act to clone the workshop git locally so that you have access to the data we will be using.\n",
    "# !git clone https://github.com/wtsi-hpag/xAIWorkshop.git\n",
    "# %cd xAIWorkshop/Code\n",
    "\n",
    "# This is the extent of fancy libraries we will be using\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "\n",
    "##load and parse the data manually. No pandas to be seen here! This function will be used repeatedly to load in our test data. \n",
    "def LoadCategoricalData(file):\n",
    "\tnames = []\n",
    "\tlabels = []\n",
    "\tdata = []\n",
    "\twith open(file) as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tentries = line.rstrip().split(' ')\n",
    "\t\t\tnames.append(entries[0])\n",
    "\t\t\tlabels.append(int(entries[1]))\n",
    "\t\t\tdata.append(np.array([ float(entries[i]) for i in range(2,len(entries))]))\n",
    "\treturn names, labels, data\n",
    "\n",
    "names, labels, data = LoadCategoricalData(\"Data/cuteness.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's a basic plotting function, feel free to write your own\n",
    "def PlotData(positions, labels,names=None):\n",
    "\tlabels = [int(b) for b in labels] ## just to make sure the data is ints we can index in, not bools\n",
    "\tcols = [\"red\",\"blue\"]\n",
    "\tx,y = zip(*positions)\n",
    "\tpt.scatter(x,y,color=np.array(cols)[labels])\n",
    "\tif names is not None:\n",
    "\t\tfor i,pos in enumerate(positions):\n",
    "\t\t\tpt.annotate(names[i],pos)\n",
    "\tpt.show()\n",
    "\n",
    "pt.title(\"True Assignments\")\n",
    "pt.xlabel(\"Size\")\n",
    "pt.ylabel(\"Fur\")\n",
    "PlotData(data,labels,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.Weights = np.array([1.0,1,-1]) ## we'll use these pre-provided weights for now\n",
    "\n",
    "\tdef Train(self,data,labels):\n",
    "\t\t##ignore this for now -- you'll fill it in later.\n",
    "\t\treturn None\n",
    "\n",
    "\tdef Predict(self,input):\n",
    "\t\t\n",
    "\t\t##this should return a value of 0 or 1, depending one how the Perceptron classifies the input vector\n",
    "\t\t## If you're feeling advanced, you might write this function to operate both on a single vector and a list-of-vectors. \n",
    "\t\treturn None ##??\n",
    "\n",
    "## Initialise, then call your Perceptron on data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Training The Perceptron\n",
    "\n",
    "You should (hopefully) now have a working Perceptron; the only issues is a) it's absolutely terrible and b) I had to give you the weights. \n",
    "\n",
    "The next challenge is to write up a training algorithm for the Perceptron. \n",
    "\n",
    "**Add a method (Train) to the Perceptron, which iterates over the dataset a number of times (you choose - when is 'enough'?) and updates the weights**\n",
    "\n",
    "Remember, given a datum $\\mathbf{x}$, a prediction $P$ and the correct label $L$ the update formula is:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{w} \\to \\mathbf{w} + r \\times \\left(L - P(\\mathbf{x}) \\right)\\mathbf{x}\n",
    "\\end{equation}\n",
    "Where $r$ is your learning rate. This formula serves to nudge the weights in a direction of the vector $\\mathbf{x}$ whenever the classifier makes a false prediction (when $L \\neq P$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = Perceptron()\n",
    "\n",
    "P.Train(data,labels) ## fill in the Train method above!\n",
    "\n",
    "## then test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: The Wrench in the Works\n",
    "\n",
    "If everything has gone to plan, you now have an algorithm that can correctly identify whether a given animal is cute or note, based on measures of their size and amount of fur. \n",
    "\n",
    "Now try running your Perceptron training & testing routines on a new dataset: ```cuteness_augmented```.\n",
    "\n",
    "**What goes wrong?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, labels, data = LoadCategoricalData(\"Data/cuteness_augmented.dat\")\n",
    "P = Perceptron()\n",
    "\n",
    "P.Train(data,labels) \n",
    "\n",
    "## then test it, and find out what has gone wrong...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Task: The Nonlinear Perceptron\n",
    "\n",
    "It is perfectly possible to generate a Perceptron which can deal with these difficult and non-linear cases: the trick is that you have to put the nonlinearity in yourself. \n",
    "\n",
    "If you have time, try the following:\n",
    "\n",
    "**Write a nonlinear perceptron which transforms the input vector (x,y) into a nonlinear vector (such as (x,y,x^2,y^3 sin(x))), and then uses this as the vector in a standard Perceptron algorithm. Does it show any improvements? How much work does it take to make the algorithm perfectly separate our cute animals from the uggos?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearPerceptron:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.Weights = None\n",
    "\t\n",
    "\tdef Train(self, data, labels):\n",
    "\t\treturn None\n",
    "\tdef Predict(self, input):\n",
    "\t\tnonLinear = self.ConvertLinearVector(input) ## this is the magic trick. Why does it work?\n",
    "\n",
    "\t\treturn None\n",
    "\tdef ConvertLinearVector(self, input):\n",
    "\t\ttransformedVector = input ## you should do something here to add in non-linearity!\n",
    "\t\treturn transformedVector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Task: The XOR Problem\n",
    "\n",
    "If you've managed to get a non-linear Perceptron working, you might be feeling pretty pleased with yourself! You've managed to (with a single node, in modern terminology) fit a highly nonlinear classifier. Let's see how far we can push it...\n",
    "\n",
    "**In the Data directory are a series of datasets called ```XOR_n``` (where ```n``` ranges from 10 to 320). Try fitting these with your non-linear classifier. What goes wrong?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, labels, data = LoadCategoricalData(\"Data/XOR_10.dat\")\n",
    "P = NonLinearPerceptron()\n",
    "P.Train(data,labels)\n",
    "## then test -- and repeat with XOR_20, 40, 80 and higher\n",
    "## What goes wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: A Multilayered Perceptron\n",
    "\n",
    "We have seen that, although incredibly easy to implement and train, a Perceptron has fundamental limitations. In their normal form, they can only split a (hyperdimensional) plane into two parts; and even when we augment our input space with hand-crafted non-linearity, there are still some problems that it cannot solve. \n",
    "\n",
    "Let us assume, therefore, that the problem was *too few decision points*. The perceptron fundamentally, makes a choice if something is above or below a certain line. But some decisions are more complex than that. How can we let the Perceptron make more sub-decisions, before coming to a final conclusion? **A network of nodes!**\n",
    "\n",
    "**Fill in the Network, Layer and Node classes such that they obey the following restrictions**\n",
    "* A Network *has* a number of layers. \n",
    "* Each layer *has* a number of nodes. \n",
    "* Each node *has* an (individual) weight vector. \n",
    "* The dimension (length) of each weight vector is equal to the number of nodes in the previous layer **plus one**. \n",
    "\t* The dimension of the weight vector in the first layer is equal to the dimension of your data **plus one**\n",
    "\t* These '+1' terms are needed for the bias. I recommend that you include the bias by augmenting your input vectors using ```augment = np.insert(vector,0,1)```\n",
    "\t* The reason for augmenting (rather than having a separate bias) will become clear when it is time to train!\n",
    "* A Network takes as input a datum, and outputs a prediction. \n",
    "* A Layer takes as input the output from the previous layer (the first layer takes the network input)\n",
    "\t* The layer then gives that vector to each node, which performs $y_i = \\mathbf{w}_i \\cdot \\begin{bmatrix} 1 \\\\ \\mathbf{x} \\end{bmatrix}$ on it. \n",
    "\t* The layer then assembles each $y_i$ into a new vector $\\mathbf{y}$. This is the output of the layer.\n",
    "* The Network takes the output of the final layer and performs the usual Perceptron classifier on it: if the result is greater than 0 it returns \"yes\", if less than 0 it returns \"no\".\n",
    "\n",
    "This sounds complex, but for now, is essentially just a complex series of handing vectors between the Network, the Layers and the Nodes. The nodes do a dot product, the Layers assemble that into a vector, and hand it off to the next layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A linear node is a simple perceptron node (without the >0 check). It's a dot product between the weights and the \n",
    "class Node:\n",
    "\tdef __init__(self):\n",
    "\t\tself.Weights = np.zeros((0,)) ## initialise an empty vector -- we cannot populate it yet because we don't know how big it is. We only know when the Network has been assembled. \n",
    "\n",
    "\t## this function informs the node of its dimensions, and is called by the Network & Layer after construction.\n",
    "\tdef Initialise(self, dimension):\n",
    "\t\treturn\n",
    "\t\n",
    "\tdef Predict(self, inputVector):\n",
    "\t\treturn None\n",
    "\n",
    "\n",
    "## A layer is a collection of nodes. The layer takes an input vector, distributes them to the nodes, and then aggregates the data.\n",
    "class Layer:\n",
    "\tdef __init__(self,nodeCount, nodeType=Node):\n",
    "\t\t#nodeCount is the number of nodes in the data, and nodeType is a class object. Note that I have defined nodeType as the *class*, not a member of that class; this lets the layer call the initialiser itself, and avoid the copying issues python can sometimes encounter.\n",
    "\n",
    "\t\t## should initialise the nodes so that they contain nodeCount copies of the nodeType class. Be careful that you initialise the node (so call nodeType()).\n",
    "\t\tself.Nodes = []\n",
    "\n",
    "\tdef Initialise(self,previousLayerDimension):\n",
    "\t\t#initialise all of the component nodes.\n",
    "\t\treturn\n",
    "\t\n",
    "\tdef Predict(self, inputVector):\n",
    "\t\t#this should pass the vector to each node, and assemble the output into a vector, which is returned\n",
    "\t\treturn\n",
    "\n",
    "\n",
    "\n",
    "## The Network class is your ``\n",
    "class Network:\n",
    "\tdef __init__(self, inputDimension, outputDimension=1):\n",
    "\t\t#input dimension is the dimension of your data -- so [x,y] is 2. \n",
    "\t\t#output dimension is the dimension of your prediction -- for now this is a binary yes/no -- so one dimension. \n",
    "\n",
    "\t\tself.Layers = [] #this is where your layers will go\n",
    "\t\tself.InputDimension = inputDimension\n",
    "\t\tself.OutputDimension = outputDimension\n",
    "\n",
    "\tdef AddLayer(self, layerObject): #append a layer object into your network. \n",
    "\t\tself.Layers.append(layerObject)\n",
    "\n",
    "\tdef Initialise(self):\n",
    "\t\t##need an initialisation function because your network doesn't know (for instance) what dimension each of the weight vectors should be until it can see the final layer structure -- remember the dimension of the n-th layer weight vectors is equal to the number of nodes in the (n-1)th layer. \n",
    "\n",
    "\n",
    "\t\t## this funciton should initialise each layer, giving them the correct information\n",
    "\n",
    "\t\treturn \n",
    "\t\n",
    "\tdef Predict(self, inputVector):\n",
    "\n",
    "\t\t#this is the feedforward pass - give the input vector to layer 0, then feed the output into the next layer, and so on and so on.\n",
    "\t\treturn \n",
    "\t\n",
    "\tdef Train(self, data,labels):\n",
    "\n",
    "\t\t#Leave this for now. It's going to be a big one!\n",
    "\t\treturn\n",
    "\n",
    "\n",
    "\t\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Your Network\n",
    "\n",
    "After writing the code to fill in all these objects, construct a network with some number of layers, each with a custom number of nodes -- I've suggested a layout below.\n",
    "\n",
    "Randomly initialise the weights of the network to values between -1 and 1 (make sure the nodes initialise independently -- the nodes on each layer should have different weights). \n",
    "\n",
    "**Test your network by scanning the space $0 < x,y < 10$. What behaviour do you observe in your network? Does it change if you alter the random initialisation?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "network = Network(2,1)\n",
    "network.AddLayer(Layer(5))\n",
    "network.AddLayer(Layer(3))\n",
    "network.AddLayer(Layer(1))\n",
    "network.Initialise()\n",
    "\n",
    "##scan over the (x,y) plane for 0 < x,y < 10 and observe the behaviour of your predictor. Can you identify its behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Solving Our Problems\n",
    "\n",
    "The problem identified above can *only* be solved by altering the way our neurons/nodes work.\n",
    "\n",
    "**Write you own custom node activation function, then retest your network. Did this fix the problematic behaviour?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you've written you original Node class well, then it suffices to make a child class which just overloads the Predict method!\n",
    "class CustomNode (Node):\n",
    "\n",
    "\tdef Predict(self, inputVector):\n",
    "\t\treturn None ##write your own function here. What functions work, which don't?\n",
    "\t\n",
    "np.random.seed(1)\n",
    "network = Network(2,1)\n",
    "network.AddLayer(Layer(5,CustomNode)) ##passed as references to the object, not as actual member-objects. Permits unique initialisations\n",
    "network.AddLayer(Layer(3,CustomNode))\n",
    "network.AddLayer(Layer(1))\n",
    "network.Initialise()\n",
    "\n",
    "##scan over the (x,y) plane for 0= < x,y < 10 and observe the behaviour of your predictor (you might have to go beyond 0->10 if your plane looks boring!). Did you fix the issues of the previous example? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Network\n",
    "\n",
    "This is the big one. This is where our maths is going to *hurt*. We now want to try and make our network learn the parameters which make it work. \n",
    "\n",
    "Unlike the Perceptron, this 'learning' is best approached through the language of calculus. Unfortunately that means that we need to move away from our hard decision boundaries (which are non-continuous and therefore non-differentiable), and move to a smooth `cost function'. \n",
    "\n",
    "This cost function is small when a set of predictions is close to being correct, and large when it is far away. \n",
    "\n",
    "If we treat our output node as producing $P$, a *probability* of being cute (0 being 100% ugly, 1 being 100% cute), then a reasonable cost function could be something along the following lines:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathcal{L} = \\sum_{\\text{data } i} \\left(P_i - L_i\\right)^2\n",
    "\\end{equation}\n",
    "This least squares cost function meets our criteria: it is zero when the predictor guesses everything correctly ($P_i = L_i$), and it is larger than zero when it is wrong, or uncertain. \n",
    "\n",
    "We now simply need to find the set of weights / network parameters which optimize this function. This is a basic gradient descent task. Each weight can be updated according to the formula:\n",
    "\\begin{equation}\n",
    "\t\\mathbf{w}_\\ell^i \\to \\mathbf{w}_\\ell^i - \\frac{\\alpha}{|\\mathbf{d}_\\ell^i|} \\times \\mathbf{d}_\\ell^i\n",
    "\\end{equation}\n",
    "Where\n",
    "\\begin{equation}\n",
    "\t\\mathbf{d} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}_\\ell^i}\n",
    "\\end{equation}\n",
    "This is basic Newtonian descent; you can equally use other methods such as Line Search or the ADAM optimiser, but whichever method you choose, ..we need the gradients. Rather than seeing our function $\\mathcal{L}$ as a function of the input data conditioned on the weights, we treat it as a function of the weights, conditioned on the data:\n",
    "\\begin{equation}\n",
    "\t\\mathcal{L} = \\mathcal{L}(\\{\\vec{w} \\}| \\vec{d})\n",
    "\\end{equation}\n",
    "Here $\\{\\vec{w}\\}$ is the set of weights on each node. We need to find:\n",
    "\\begin{equation}\n",
    "\t\\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}_i} = 2\\sum_i  \\left(P_i - L_i\\right) \\frac{\\partial P_i}{\\partial \\vec{w}}\n",
    "\\end{equation}\n",
    "\n",
    "### Working Your Way Backwards\n",
    "\n",
    "In the notes, I show that the following equations can be used to compute the gradients:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial C_q}{\\partial \\vec{w}_\\ell^i} &= \\frac{\\partial C_q}{\\partial y_\\ell^i} \\tilde{\\vec{x}}_{\\ell-1}\n",
    "\\\\\n",
    "\\frac{\\partial C_q}{\\partial y_\\ell^i} & = \\begin{cases} \\frac{\\partial C_q}{\\partial P_q^i} \\times d_N(y_N^i) & \\text{ if $\\ell = N-1$}\n",
    "\\\\\n",
    "d_\\ell(y_\\ell^i) \\sum_{j}  \\left[ \\vec{w}_{\\ell+1}^j \\right]_{i+1} \\frac{\\partial C_q}{\\partial y_{\\ell+1}^j}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "This requires starting at the final layer ($\\ell = N-1$) and working your way backwards through the network, using the previous layer's results to inform the next. For this reason it is known as *backpropagation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Writing a Trainer\n",
    "\n",
    "If you are confident in your coding abilities and your grasp of the mathematics, then I encourage you to try and adapt your existing Network/Layer/Nodes by including a Training operation which executes the chain rule equations explained above. \n",
    "\n",
    "I am, however, aware that this might seem a daunting task. \n",
    "\n",
    "I have therefore prepared an alternative route: you may use this to augment your attempts to write your own backpropagation, or you might rely solely on this. It doesn't matter: all that matters is that you spend some time now trying to draw the lines between the arcane theory I have explained, and the bare-metal code which is being executed.\n",
    "\n",
    "\n",
    "### Code Jigsaw\n",
    "\n",
    "The following code outlines the *barest possible bones* of a class structure for a Network, Layer, Node plus a handful of other useful objects. \n",
    "\n",
    "You will note, however, that the code contains only member function names and arguments. There is no code written here. \n",
    "\n",
    "In the file [```Data/jigsaw.py```](Data/jigsaw.py) you will find a number of functions. They have been anonymised: all variables reduced to single letters and names replaced with generic ones: all function calls have been replaced with ```member_x()```.\n",
    "\n",
    "Using the class structure in front of you, and your new knowledge of how neural networks function, disentangle this code and use it to write your own version which is -- hopefully -- slightly less obtuse and difficult to read.\n",
    "\n",
    "**(Not all functions return values -- I just added return calls so that this code block wouldn't hurl errors at me due to empty code!)**\n",
    "\n",
    "#### If you get stuck\n",
    "\n",
    "As well as the completed code found in the Solutions notebook, there are also some modified versions of the jigsaw pieces in [```Hints/jigsaw_hints.py```](Hints/jigsaw_hints.py), which have slightly less obfuscation, or additional comments which should help inform you what is going on in each method.\n",
    "\n",
    "### Optional Challenges \n",
    "\n",
    "In ```Data/jigsaw_advanced``` there are some more complex functions....what are they doing? Do they improve or degrade your performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction:\n",
    "\tdef Compute(self,prediction,label):\n",
    "\t\treturn\n",
    "\tdef Gradient(self,prediction,label):\n",
    "\t\treturn\n",
    "\t\n",
    "class Node:\n",
    "\n",
    "\tdef Initialise(self, dimension):\n",
    "\t\treturn\n",
    "\tdef Predict(self, inputVector): \n",
    "\t\treturn\n",
    "\tdef ActivationFunction(self, inputValue): \n",
    "\t\treturn\n",
    "\tdef ActivationGradient(self, inputValue): \n",
    "\t\treturn\n",
    "\t\n",
    "\tdef UpdateGradient(self, dLdY, inputVector): \n",
    "\t\treturn \n",
    "\t \n",
    "\tdef UpdateWeights(self,alpha):\n",
    "\t\treturn \n",
    "\n",
    "class SigmoidNode (Node):\n",
    "\n",
    "\tdef ActivationFunction(self, y):\n",
    "\t\treturn\n",
    "\t\t\n",
    "\tdef ActivationGradient(self, y):\n",
    "\t\treturn \n",
    "\t\n",
    "class ReluNode (Node): #technically this is leaky, but it makes the gradients easier.\n",
    "\n",
    "\tdef ActivationFunction(self, y):\n",
    "\t\treturn \n",
    "\t\n",
    "\tdef ActivationGradient(self, y):\n",
    "\t\treturn\n",
    "\n",
    "class Layer:\n",
    "\tdef __init__(self,nodeCount, nodeType=Node):\n",
    "\t\treturn\n",
    "\t\t\n",
    "\tdef Initialise(self,previousLayerDimension):\n",
    "\t\treturn\n",
    "\n",
    "\tdef Predict(self, inputVector):\n",
    "\t\treturn \n",
    "\n",
    "\tdef MiddleLayerGradient(self,layerAbove,inputVector):\n",
    "\t\treturn\n",
    "\n",
    "\tdef FinalLayerGradient(self,dLdP,inputVector):\n",
    "\t\treturn\n",
    "\n",
    "\tdef UpdateWeights(self,alpha):\n",
    "\t\treturn\n",
    "\n",
    "\t\n",
    "class Network:\n",
    "\tdef __init__(self, inputDimension, outputDimension=1):\n",
    "\t\treturn\n",
    "\n",
    "\tdef AddLayer(self, layerObject):\n",
    "\t\treturn\n",
    "\n",
    "\tdef Initialise(self):\n",
    "\t\treturn\n",
    "\t\n",
    "\tdef Predict(self, inputVector):\n",
    "\t\treturn \n",
    "\t\n",
    "\tdef Train(self, data,labels,costFunc = CostFunction(),Nsteps=1000):\n",
    "\t\t\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Your Network\n",
    "\n",
    "Once you have assembled your network, try running it on the following datasets:\n",
    "\n",
    "1. ```Cuteness.dat``` - how does it handle an easily linearly separable dataset?\n",
    "2. ```Cuteness_augmented.dat``` - can it handle the non-linear cases?\n",
    "3. ```complex_classifier.dat``` - how well can it handle this highly non-linear case with multiple islands?\n",
    "4. ```complex_function.dat``` - this case is not a classifier, but an emulator; you are trying to predict a function which has numerical outputs, not a binary value. Is this easier or more difficult?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Need to use this on complex_function as the values are non-integers.\n",
    "def LoadNumericalData(file):\n",
    "\tnames = []\n",
    "\tlabels = []\n",
    "\tdata = []\n",
    "\twith open(file) as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tentries = line.rstrip().split(' ')\n",
    "\t\t\tnames.append(entries[0])\n",
    "\t\t\tlabels.append(float(entries[1]))\n",
    "\t\t\tdata.append(np.array([ float(entries[i]) for i in range(2,len(entries))]))\n",
    "\treturn names, labels, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Additional Challenges\n",
    "\n",
    "The default Cost function provided is the Least Squares function. Why might this not be best suited for the classifier tasks?\n",
    "\n",
    "Write a custom cost function (and it's derivative!) and test if it improves the performance of your function.\n",
    "\n",
    "(Hint: Classifiers are best thought of as operating in probability space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "Your network should (fingers crossed) work adequately on these datasets - demonstrating that the goings on of your favourite machine learning toolbox are not witchcraft. \n",
    "\n",
    "What would be your main thought about the performance of this network, versus a production ML toolbox?\n",
    "\n",
    "What else would you need to include for this to be properly 'functional'?\n",
    "\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "The [provided notes](../Notes/Notes.pdf) go into a lot more detail regarding the underlying mathematics and theory, as well as a handful of musings on commonly discussed aspects of ML such as overfitting and regularisation -- particularly in the context of the mathematical framework we have established here. I encourage you to go and read those, as they might shed some light on some of the bigger problems I personally feel stand in the way of Explainable AI, and the challenges that must be overcome if we truly want to understand why these networks make the decisions that they do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
