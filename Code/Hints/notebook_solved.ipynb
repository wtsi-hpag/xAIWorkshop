{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Solutions\n",
    "### (to the xAI workshop, by Jack Fraser-Govil)\n",
    "\n",
    "This document serves as the solutions sheet to the [main workshop notebook](../xai_workshop.ipynb). It is an almost exact duplicate of that file, with the code sections filled in such that the tasks are completed. \n",
    "\n",
    "##### However\n",
    "\n",
    "Whilst it is certainly possible for you to copy and paste the entire contents of this file into your worksheet, finish all the tasks in 10 minutes, and then go for an early lunch, the entire purpose of this workshop is for you to come away with the satisfaction of having written your own learning networks from scratch. We are, by definition, reinventing the wheel here: everything we make will be orders of magnitude worse than pytorch or tensorflow; the principle is that **you learn best by doing it yourself**.\n",
    "\n",
    "If you're going to use the 'cheat sheet', then try, first of all, to simply look at it to understand what it is doing, and then implement your own version. Don't directly copy whole chunks of code you don't understand, as that sort of defeats the point of today's efforts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: The Pretrained Perceptron\n",
    "\n",
    "Just to get the juices flowing, let's get started with an easy project.\n",
    "\n",
    "**In the ```Data``` directory is a file ```cuteness.tex```, which contains (non-dimensional) measures of size and furriness of some animals, as well as some human-curated labels (1 for cute, 0 for note cute). Please write a method which performs Perceptron classification, and use it to identify which of these creatures the model currently mislabels.**  \n",
    "\n",
    "I've provided some basic functions and a class framework for you to work from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This cell is unmodified form the version in the workshop notebook, it's just here so that this notebook can run as a standalone\n",
    "\n",
    "# !git clone https://github.com/wtsi-hpag/xAIWorkshop.git\n",
    "# %cd xAIWorkshop/Code\n",
    "\n",
    "# This is the extent of fancy libraries we will be using\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "\n",
    "##load and parse the data manually. No pandas to be seen here! This function will be used repeatedly to load in our test data. \n",
    "def LoadCategoricalData(file):\n",
    "\tnames = []\n",
    "\tlabels = []\n",
    "\tdata = []\n",
    "\twith open(file) as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tentries = line.rstrip().split(' ')\n",
    "\t\t\tnames.append(entries[0])\n",
    "\t\t\tlabels.append(int(entries[1]))\n",
    "\t\t\tdata.append(np.array([ float(entries[i]) for i in range(2,len(entries))]))\n",
    "\treturn names, labels, data\n",
    "\n",
    "names, labels, data = LoadCategoricalData(\"../Data/cuteness.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This cell is unmodified form the version in the workshop notebook, it's just here so that this notebook can run as a standalone\n",
    "\n",
    "def PlotData(positions, labels,names=None):\n",
    "\tlabels = [int(b) for b in labels] ## just to make sure the data is ints we can index in, not bools\n",
    "\tcols = [\"red\",\"blue\"]\n",
    "\tx,y = zip(*positions)\n",
    "\tpt.scatter(x,y,color=np.array(cols)[labels])\n",
    "\tif names is not None:\n",
    "\t\tfor i,pos in enumerate(positions):\n",
    "\t\t\tpt.annotate(names[i],pos)\n",
    "\tpt.show()\n",
    "\n",
    "pt.title(\"True Assignments\")\n",
    "pt.xlabel(\"Size\")\n",
    "pt.ylabel(\"Fur\")\n",
    "PlotData(data,labels,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.Weights = np.array([1,1,-1]) ## we'll use these pre-provided weights for now\n",
    "\n",
    "\n",
    "\t#####################################\n",
    "\tdef Predict(self,input):\n",
    "\t\t\n",
    "\t\t## this is just a fancy bit so that it can (inefficiently) loop over an input list and return a list of predictions.\n",
    "\t\tif type(input) is list:\n",
    "\t\t\tout = []\n",
    "\t\t\tfor r in input:\n",
    "\t\t\t\t# print(r)\n",
    "\t\t\t\tout.append(self.Predict(r))\n",
    "\t\t\treturn out\n",
    "\t\t\n",
    "\t\t## this is the meat of the predictor\n",
    "\t\treturn 1.0 * (np.dot(self.Weights,np.insert(input,0,1)) > 0)\n",
    "\t\n",
    "\t\t## this is a less one-line version of that same code:\n",
    "\t\t# augmentedVector = np.inserp(input,0,1) ##augment to add in the bias term -- weights have len = 3, but input is (x,y).\n",
    "\t\t# dot = np.dot(self.Weights,augmentedVector)\n",
    "\t\t# label = dot > 0 ## this is a boolean value\n",
    "\t\t# return 1.0 * label ##convert to 0.0 (false) or 1.0 (true) because plotting libraries don't like booleans! \n",
    "\n",
    "\t#####################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a quick analysis function which we can re-use.\n",
    "def AnalysePerceptron(Perceptron,data,labels,names=None,plotting = False):\n",
    "\n",
    "\t\n",
    "\n",
    "\t# this generates an array of predicted labels\n",
    "\tpredict_labels = np.round(Perceptron.Predict(data)) ## rounding is for later!\n",
    "\t#this works out which ones are incorrect then prints them\n",
    "\tmisslabels = predict_labels != np.array(labels)\n",
    "\tif names is not None:\n",
    "\t\tmissed_names = np.array(names)[misslabels]\n",
    "\t\tprint(\"Mislabelled entities are:\", missed_names)\n",
    "        \n",
    "        \n",
    "\tprint(f\"Success Rate= {round(100*(1.0 - np.sum(misslabels) * 1.0/len(names)))}%\")\n",
    "\n",
    "\tif plotting:\n",
    "\t\t## this is an anlytical means of converting the weights into y = mx + c\n",
    "\t\tx = np.linspace(0,10,10)\n",
    "\t\tw = P.Weights\n",
    "\t\ty = -w[1]*x/w[2] - w[0]/w[2]\n",
    "\t\tpt.plot(x,y)\n",
    "\t\t#this then plots the data with blue = correct predictions, red = false predictions\n",
    "\t\tPlotData(data,(predict_labels==np.array(labels)),names)\n",
    "\t\n",
    "\n",
    "P = Perceptron()\n",
    "AnalysePerceptron(P,data,labels,names,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Training The Perceptron\n",
    "\n",
    "You should (hopefully) now have a working Perceptron; the only issues is a) it's absolutely terrible and b) I had to give you the weights. \n",
    "\n",
    "The next challenge is to write up a training algorithm for the Perceptron. \n",
    "\n",
    "**Add a method (Train) to the Perceptron, which iterates over the dataset a number of times (you choose - when is 'enough'?) and updates the weights**\n",
    "\n",
    "Remember, given a datum $\\mathbf{x}$, a prediction $P$ and the correct label $L$ the update formula is:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{w} \\to \\mathbf{w} + r \\times \\left(L - P(\\mathbf{x}) \\right)\\mathbf{x}\n",
    "\\end{equation}\n",
    "Where $r$ is your learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I'm fully redefining the Perceptron class; just so that these notebook is iteratively solved. You don't have to do that!\n",
    "class Perceptron:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.Weights = np.array([1.0,1,-1]) ## we'll use these pre-provided weights for now\n",
    "\n",
    "\tdef Predict(self,input):\n",
    "\t\t\n",
    "\t\tif type(input) is list:\n",
    "\t\t\tout = []\n",
    "\t\t\tfor r in input:\n",
    "\t\t\t\t# print(r)\n",
    "\t\t\t\tout.append(self.Predict(r))\n",
    "\t\t\treturn out\n",
    "\t\n",
    "\t\t## this only performs the augmentation if the input dimensions are of the incorrect size; this permits pre-augmenting your data. \n",
    "\t\tif len(input) == len(self.Weights):\n",
    "\t\t\treturn 1.0 * (np.dot(self.Weights,input) > 0)\n",
    "\t\telse:\n",
    "\t\t\treturn 1.0 * (np.dot(self.Weights,np.insert(input,0,1)) > 0)\n",
    "\n",
    "\tdef Train(self, data,labels,fullEpochs = 100):\n",
    "\n",
    "\t\trate = 0.01\n",
    "\t\taugmentedData = []\n",
    "\t\tfor d in data:\n",
    "\t\t\taugmentedData.append(np.insert(d,0,1)) ## preaugment my data so that I don't waste CPU cycles doing it over and over again!\n",
    "\n",
    "\t\tids = np.arange(len(data))\n",
    "\t\tfor l in range(fullEpochs):\n",
    "\t\t\tnoMistakes = True\n",
    "\t\t\tnp.random.shuffle(ids) ## go through the data in a different order each time. Helps convergence\n",
    "\t\t\tfor r in range(len(data)):\n",
    "\n",
    "\t\t\t\tP = self.Predict(augmentedData[ids[r]])\n",
    "\n",
    "\t\t\t\tupdateSize = rate*(labels[ids[r]]-P) \n",
    "\t\t\t\tupdate = updateSize * augmentedData[ids[r]]\n",
    "\t\t\t\tself.Weights +=  update\n",
    "\t\t\t\tif (P!= labels[ids[r]]):\n",
    "\t\t\t\t\tnoMistakes = False\n",
    "\t\t\tif noMistakes:\n",
    "\t\t\t\tprint(\"Converged early, after\",l,\"epochs\")\n",
    "\t\t\t\treturn\n",
    "\n",
    "P = Perceptron()\n",
    "P.Train(data,labels,500)\n",
    "AnalysePerceptron(P,data,labels,names,True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: The Wrench in the Works\n",
    "\n",
    "If everything has gone to plan, you now have an algorithm that can correctly identify whether a given animal is cute or note, based on measures of their size and amount of fur. \n",
    "\n",
    "Now try running your Perceptron training & testing routines on a new dataset: ```cuteness_augmented```.\n",
    "\n",
    "**What goes wrong?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_aug, labels_aug, data_aug = LoadCategoricalData(\"../Data/cuteness_augmented.dat\")\n",
    "\n",
    "P = Perceptron()\n",
    "\n",
    "sum = 0\n",
    "for train in [0,100,200,300,1000,195,107,206]:\n",
    "\tP.Train(data_aug,labels_aug,train)\n",
    "\tpt.title(f\"After {sum} Epochs\")\n",
    "\tAnalysePerceptron(P,data_aug,labels_aug,names_aug,True)\n",
    "\tsum +=train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Task: The Nonlinear Perceptron\n",
    "\n",
    "It is perfectly possible to generate a Perceptron which can deal with these difficult and non-linear cases: the trick is that you have to put the nonlinearity in yourself. \n",
    "\n",
    "If you have time, try the following:\n",
    "\n",
    "**Write a nonlinear perceptron which transforms the input vector (x,y) into a nonlinear vector (such as (x,y,x^2,y^3 sin(x))), and then uses this as the vector in a standard Perceptron algorithm. Does it show any improvements? How much work does it take to make the algorithm perfectly separate our cute animals from the uggos?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm #ok I lied about no more libraries, but I wanted a progress bar\n",
    "class NonlinearPerceptron:\n",
    "\n",
    "\tdef __init__(self,maxPower=1):\n",
    "\t\tself.Power = maxPower\n",
    "\t\tself.nDim = int(((maxPower+2) * (maxPower + 1))/2)\n",
    "\t\tself.Weights = np.zeros((self.nDim,)) ## we'll use these pre-provided weights for now\n",
    "\t\tself.Weights[0] = 0.5\n",
    "\t\tself.Weights[1] = 1\n",
    "\t\tself.Weights[2] = -1\n",
    "\n",
    "\t\t# print(self.Weights)\n",
    "\tdef Predict(self,input,vectorfied=False):\n",
    "\t\t\n",
    "\t\tif type(input) is list:\n",
    "\t\t\tout = []\n",
    "\t\t\tfor r in input:\n",
    "\t\t\t\t# print(r)\n",
    "\t\t\t\tout.append(self.Predict(r))\n",
    "\t\t\treturn out\n",
    "\t\tif vectorfied:\n",
    "\t\t\treturn 1.0 * (np.dot(self.Weights,input) > 0)\n",
    "\t\telse:\n",
    "\t\t\tvec = self.Vectorfy(input)\n",
    "\t\t\treturn 1.0 * (np.dot(self.Weights,vec) > 0)\n",
    "\n",
    "\t## this is the non-linear magic bit. It generates the set of all 2D polybinomials up to the specified order.\n",
    "\tdef Vectorfy(self,input):\n",
    "\t\t\n",
    "\t\toutput = np.zeros((self.nDim,))\n",
    "\t\toutput[0] = 1\n",
    "\t\ts = 1\n",
    "\t\t\n",
    "\t\tfor power in range(1,self.Power+1):\n",
    "\t\t\tfor q in range(power+1):\n",
    "\t\t\t\toutput[s] = input[0]**q * input[1]**(power-q)\n",
    "\t\t\t\ts+= 1\n",
    "\t\treturn np.array(output)\n",
    "\n",
    "\n",
    "\tdef Train(self, data,labels,fullEpochs = 100):\n",
    "\t\tvectorfied = [self.Vectorfy(d) for d in data]\n",
    "\n",
    "\n",
    "\t\trate = 0.5\n",
    "\t\t##this is a bit of a hack (and a hint at better optimisation routines!)\n",
    "\t\t## Because my vector has higher powers of x, the optimiser wobbles incoherently if the powers are x**10 as it's simultaenoulsy trying to balance +/- 1 and +/- a billion\n",
    "\t\t## this slows down the learning at the higher power parameters, letting it be much less sensitive to them\n",
    "\t\trates = np.zeros((self.nDim))\n",
    "\t\ts = 0\n",
    "\t\tfor power in range(self.Power):\n",
    "\t\t\tfor r in range(power+1):\n",
    "\t\t\t\trates[s] = rate / 5**power\n",
    "\t\t\t\ts+= 1\n",
    "\n",
    "\t\tids = np.arange(len(data))\n",
    "\t\tfor l in tqdm(range(fullEpochs)):\n",
    "\t\t\tnp.random.shuffle(ids)\n",
    "\t\t\tnoMistakes = True\n",
    "\t\t\tfor r in range(len(data)):\n",
    "\t\t\t\tP = self.Predict(vectorfied[ids[r]],True)\n",
    "\n",
    "\t\t\t\tupdate = rates*(labels[ids[r]]-P) * vectorfied[ids[r]]\n",
    "\t\t\t\tself.Weights +=  update\n",
    "\t\t\t\tif (P!= labels[ids[r]]):\n",
    "\t\t\t\t\tnoMistakes = False\n",
    "\t\t\tif noMistakes:\n",
    "\t\t\t\tprint(\"Converged early, after\",l,\"epochs\")\n",
    "\t\t\t\treturn\n",
    "\n",
    "\n",
    "## this function lets me plot surface maps of where the predictor has drawn its decision boundaries.\n",
    "def ScanClassifierSpace(Predictor,xmin,xmax,ymin,ymax,resolution,dotSize =4,cmapping=True):\n",
    "\tx = np.linspace(xmin,xmax,resolution)\n",
    "\ty = np.linspace(ymin,ymax,resolution)\n",
    "\n",
    "\tN = len(x)*len(y)\n",
    "\txs = np.zeros((N,))\n",
    "\tys = np.zeros((N,))\n",
    "\tzs = np.zeros((N,))\n",
    "\ts= 0\n",
    "\tz = np.zeros((len(y),len(x)))\n",
    "\tfor i,xp in enumerate(x):\n",
    "\t\tfor j,yp in enumerate(y):\n",
    "\t\t\tl = Predictor.Predict(np.array([xp,yp]))\n",
    "\t\t\txs[s] = xp\n",
    "\t\t\tys[s] = yp\n",
    "\t\t\tzs[s] = float(l)\n",
    "\t\t\ts+=1\n",
    "\n",
    "\tif cmapping:\n",
    "\t\tpt.scatter(xs,ys,dotSize,zs,cmap='RdBu')\n",
    "\telse:\n",
    "\t\tpt.scatter(xs,ys,dotSize,zs)\n",
    "\n",
    "\n",
    "P = NonlinearPerceptron(7)\n",
    "P.Train(data_aug,labels_aug,120000)\n",
    "\n",
    "AnalysePerceptron(P,data_aug,labels_aug,names_aug,False)\n",
    "\n",
    "\n",
    "ScanClassifierSpace(P,0,10,0,6,100)\n",
    "pt.xlabel(\"Size\")\n",
    "pt.ylabel(\"Fur\")\n",
    "pt.clim([-0.2,1.2])\n",
    "PlotData(data_aug,np.array(labels_aug),names_aug)\n",
    "\n",
    "pt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Task: The XOR Problem\n",
    "\n",
    "If you've managed to get a non-linear Perceptron working, you might be feeling pretty pleased with yourself! You've managed to (with a single node, in modern terminology) fit a highly nonlinear classifier.\n",
    "\n",
    "**In the Data directory are a series of datasets called ```XOR_n``` (where ```n``` ranges from 10 to 1000). Try fitting these with your non-linear classifier. What goes wrong?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, labels, data = LoadCategoricalData(\"../Data/XOR_160.dat\")\n",
    "P = NonlinearPerceptron(6)\n",
    "P.Train(data,labels,50)\n",
    "\n",
    "ScanClassifierSpace(P,0,2,0,2,100)\n",
    "pt.plot([0,2],[1,1],'k')\n",
    "pt.plot([1,1],[0,2],'k')\n",
    "pt.clim([-0.2,1.2])\n",
    "pt.show()\n",
    "PlotData(data,np.array(labels)) # plot on different axis because otherwise it gets a bit dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: A Multilayered Perceptron\n",
    "\n",
    "We have seen that, although incredibly easy to implement and train, a Perceptron has fundamental limitations. In their normal form, they can only split a (hyperdimensional) plane into two parts; and even when we augment our input space with hand-crafted non-linearity, there are still some problems that it cannot solve. \n",
    "\n",
    "Let us assume, therefore, that the problem was *too few decision points*. The perceptron fundamentally, makes a choice if something is above or below a certain line. But some decisions are more complex than that. How can we let the Perceptron make more sub-decisions, before coming to a final conclusion? **A network of nodes!**\n",
    "\n",
    "**Fill in the Network, Layer and Node classes such that they obey the following restrictions**\n",
    "* A Network *has* a number of layers. \n",
    "* Each layer *has* a number of nodes. \n",
    "* Each node *has* an (individual) weight vector. \n",
    "* The dimension (length) of each weight vector is equal to the number of nodes in the previous layer. \n",
    "\t* The dimension of the weight vector in the first layer is equal to the dimension of your data\n",
    "* A Network takes as input a datum, and outputs a prediction. \n",
    "* A Layer takes as input the output from the previous layer (the first layer takes the network input)\n",
    "\t* The layer then gives that vector to each node, which performs $y_i = \\mathbf{w}_i \\cdot \\begin{bmatrix} 1 \\\\ \\mathbf{x} \\end{bmatrix}$ on it. \n",
    "\t* The layer then assembles each $y_i$ into a new vector $\\mathbf{y}$. This is the output of the layer.\n",
    "* The Network takes the output of the final layer and performs the usual Perceptron classifier on it: if the result is greater than 0 it returns \"yes\", if less than 0 it returns \"no\".\n",
    "\n",
    "This sounds complex, but for now, is essentially just a complex series of handing vectors between the Network, the Layers and the Nodes. The nodes do a dot product, the Layers assemble that into a vector, and hand it off to the next layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A linear node is a simple perceptron node (without the >0 check). It's a dot product between the weights and the \n",
    "class Node:\n",
    "\tdef __init__(self):\n",
    "\t\tself.Weights = np.zeros((0,)) ## initialise an empty vector -- we cannot populate it yet because we don't know how big it is. We only know when the Network has been assembled. \n",
    "\n",
    "\t## this function informs the node of its dimensions, and is called by the Network & Layer after construction.\n",
    "\tdef Initialise(self, dimension):\n",
    "\t\tself.Weights = 2*(np.random.random((dimension,)) - 0.5) # randomly initialises to a value between -1 and 1.\n",
    "\t\n",
    "\tdef Predict(self, inputVector): ##I assume that the layer will augment for me, to save some CPU cycles\n",
    "\t\treturn np.dot(self.Weights,inputVector)\n",
    "\n",
    "\n",
    "## A layer is a collection of nodes. The layer takes an input vector, distributes them to the nodes, and then aggregates the data.\n",
    "class Layer:\n",
    "\tdef __init__(self,nodeCount, nodeType=Node):\n",
    "\t\t#nodeCount is the number of nodes in the data, and nodeType is a class object. Note that I have defined nodeType as the *class*, not a member of that class; this lets the layer call the initialiser itself, and avoid the copying issues python can sometimes encounter.\n",
    "\n",
    "\t\t## should initialise the nodes so that they contain nodeCount copies of the nodeType class. Be careful that you initialise the node (so call nodeType(), don't just put nodeType into your vector).\n",
    "\t\tself.Nodes = []\n",
    "\t\tfor i in range(nodeCount):\n",
    "\t\t\tself.Nodes.append(nodeType())\n",
    "\t\tself.OutputVector = np.zeros((nodeCount,)) #rather than returning a vector and passing it around, I store it as a member of the class. Then I just slot things in rather than creating a new vector each loop. Saves some CPU cycles.\n",
    "\t\t\n",
    "\tdef Initialise(self,previousLayerDimension):\n",
    "\t\tfor node in self.Nodes:\n",
    "\t\t\tnode.Initialise(previousLayerDimension)\n",
    "\n",
    "\tdef Predict(self, inputVector):\n",
    "\t\taugmentedVector = np.insert(inputVector,0,1)\n",
    "\t\tfor i,node in enumerate(self.Nodes):\n",
    "\t\t\tself.OutputVector[i] = node.Predict(augmentedVector)\n",
    "\n",
    "\n",
    "## The Network class is your ``\n",
    "class Network:\n",
    "\tdef __init__(self, inputDimension, outputDimension=1):\n",
    "\t\t#input dimension is the dimension of your data -- so [x,y] is 2. \n",
    "\t\t#output dimension is the dimension of your prediction -- for now this is a binary yes/no -- so one dimension. \n",
    "\n",
    "\t\tself.Layers = [] #this is where your layers will go\n",
    "\t\tself.InputDimension = inputDimension\n",
    "\t\tself.OutputDimension = outputDimension\n",
    "\n",
    "\tdef AddLayer(self, layerObject): #append a layer object into your network. \n",
    "\t\tself.Layers.append(layerObject)\n",
    "\n",
    "\tdef Initialise(self):\n",
    "\t\t##need an initialisation function because your network doesn't know (for instance) what dimension each of the weight vectors should be until it can see the final layer structure -- remember the dimension of the n-th layer weight vectors is equal to the number of nodes in the (n-1)th layer. \n",
    "\n",
    "\n",
    "\t\t## this funciton should initialise each layer, giving them the correct information\n",
    "\t\tpreviousLayerDimension = self.InputDimension\n",
    "\t\tfor layer in self.Layers:\n",
    "\t\t\tlayer.Initialise(previousLayerDimension+1) ## the +1 is for the `bias` term. If you're manually including the bias, you don't need this\n",
    "\t\t\tpreviousLayerDimension = len(layer.Nodes)\n",
    "\t\n",
    "\t\t##then check that my dimensions make sense!\n",
    "\t\tif previousLayerDimension != self.OutputDimension:\n",
    "\t\t\tprint(\"ERROR! Your network output dimension is not equal to the final layer dimension\")\n",
    "\n",
    "\n",
    "\n",
    "\tdef Predict(self, inputVector):\n",
    "\n",
    "\t\tself.Layers[0].Predict(inputVector)\n",
    "\t\tfor i in range(1,len(self.Layers)):\n",
    "\t\t\tself.Layers[i].Predict(self.Layers[i-1].OutputVector)\n",
    "\t\tfinalValue = self.Layers[-1].OutputVector\n",
    "\t\t##techincally this is a hack because I know that the dimension is 1 -- this is just for the curent challenge!\n",
    "\t\treturn int(finalValue > 0)\n",
    "\t\n",
    "\tdef Train(self, data,labels):\n",
    "\n",
    "\t\t#Leave this for now. It's going to be a big one!\n",
    "\t\treturn\n",
    "\n",
    "\n",
    "\t\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "\n",
    "network = Network(2,1)\n",
    "network.AddLayer(Layer(5))\n",
    "network.AddLayer(Layer(3))\n",
    "network.AddLayer(Layer(1))\n",
    "network.Initialise()\n",
    "\n",
    "## plot these out of bounds just to get the legend right!\n",
    "pt.scatter([-100],[-100],5,'r',label='P = 0')\n",
    "pt.scatter([-100],[100],5,'b',label='P=1')\n",
    "\n",
    "\n",
    "ScanClassifierSpace(network,-10,10,-10,10,200)\n",
    "pt.legend()\n",
    "pt.xlim([-10,10])\n",
    "pt.ylim([-10,10])\n",
    "pt.xlabel(\"Size\")\n",
    "pt.ylabel(\"Fur\")\n",
    "pt.clim([-0.2,1.2])\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Solving Our Problems\n",
    "\n",
    "The problem identified above can *only* be solved by altering the way our neurons/nodes work.\n",
    "\n",
    "**Write you own custom node activation function, then retest your network. Did this fix the problematic behaviour?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidNode (Node):\n",
    "\n",
    "\tdef Predict(self, inputVector):\n",
    "\t\ta = np.dot(self.Weights,inputVector)\n",
    "\t\treturn 1.0/(1.0 + np.exp(-a))\n",
    "\t\n",
    "np.random.seed(1)\n",
    "network = Network(2,1)\n",
    "network.AddLayer(Layer(5,SigmoidNode))\n",
    "network.AddLayer(Layer(3,SigmoidNode))\n",
    "network.AddLayer(Layer(1))\n",
    "network.Initialise()\n",
    "\n",
    "## plot these out of bounds just to get the legend right!\n",
    "pt.scatter([-100],[-100],5,'r',label='P = 0')\n",
    "pt.scatter([-100],[100],5,'b',label='P=1')\n",
    "\n",
    "print(network.Predict([0,0]))\n",
    "## then do my scan\n",
    "ScanClassifierSpace(network,-10,10,-10,10,200)\n",
    "pt.legend()\n",
    "pt.xlim([-10,10])\n",
    "pt.ylim([-10,10])\n",
    "pt.xlabel(\"Size\")\n",
    "pt.ylabel(\"Fur\")\n",
    "pt.clim([-0.2,1.2])\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Network\n",
    "\n",
    "This is the big one. This is where our maths is going to *hurt*. We now want to try and make our network learn the parameters which make it work. \n",
    "\n",
    "Unlike the Perceptron, this 'learning' is best approached through the language of calculus. Unfortunately that means that we need to move away from our hard decision boundaries (which are non-continuous and therefore non-differentiable), and move to a smooth `cost function'. \n",
    "\n",
    "This cost function is small when a set of predictions is close to being correct, and large when it is far away. \n",
    "\n",
    "If we treat our output node as producing $P$, a *probability* of being cute (0 being 100% ugly, 1 being 100% cute), then a reasonable cost function could be something along the following lines:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathcal{L} = \\sum_{\\text{data } i} \\left(P_i - L_i\\right)^2\n",
    "\\end{equation}\n",
    "This least squares cost function meets our criteria: it is zero when the predictor guesses everything correctly ($P_i = L_i$), and it is larger than zero when it is wrong, or uncertain. \n",
    "\n",
    "We now simply need to find the set of weights / network parameters which optimize this function. This is a basic gradient descent task. Each weight can be updated according to the formula:\n",
    "\\begin{equation}\n",
    "\t\\mathbf{w}_\\ell^i \\to \\mathbf{w}_\\ell^i - \\frac{\\alpha}{|\\mathbf{d}_\\ell^i|} \\times \\mathbf{d}_\\ell^i\n",
    "\\end{equation}\n",
    "Where\n",
    "\\begin{equation}\n",
    "\t\\mathbf{d} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}_\\ell^i}\n",
    "\\end{equation}\n",
    "This is basic Newtonian descent; you can equally use other methods such as Line Search or the ADAM optimiser, but whichever method you choose, ..we need the gradients. Rather than seeing our function $\\mathcal{L}$ as a function of the input data conditioned on the weights, we treat it as a function of the weights, conditioned on the data:\n",
    "\\begin{equation}\n",
    "\t\\mathcal{L} = \\mathcal{L}(\\{\\vec{w} \\}| \\vec{d})\n",
    "\\end{equation}\n",
    "Here $\\{\\vec{w}\\}$ is the set of weights on each node. We need to find:\n",
    "\\begin{equation}\n",
    "\t\\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}_i} = 2\\sum_i  \\left(P_i - L_i\\right) \\frac{\\partial P_i}{\\partial \\vec{w}}\n",
    "\\end{equation}\n",
    "\n",
    "### Working Your Way Backwards\n",
    "\n",
    "In the notes, I show that the following equations can be used to compute the gradients:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial C_q}{\\partial \\vec{w}_\\ell^i} &= \\frac{\\partial C_q}{\\partial y_\\ell^i} \\tilde{\\vec{x}}_{\\ell-1}\n",
    "\\\\\n",
    "\\frac{\\partial C_q}{\\partial y_\\ell^i} & = \\begin{cases} \\frac{\\partial C_q}{\\partial P_q^i} \\times d_N(y_N^i) & \\text{ if $\\ell = N-1$}\n",
    "\\\\\n",
    "d_\\ell(y_\\ell^i) \\sum_{j}  \\left[ \\vec{w}_{\\ell+1}^j \\right]_{i+1} \\frac{\\partial C_q}{\\partial y_{\\ell+1}^j}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "This requires starting at the final layer ($\\ell = N-1$) and working your way backwards through the network, using the previous layer's results to inform the next. For this reason it is known as *backpropagation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction:\n",
    "\t\t\n",
    "\tdef Compute(self,prediction,label):\n",
    "\t\tif type(prediction) is list:\n",
    "\t\t\tv = 0\n",
    "\t\t\tfor p,i in enumerate(prediction):\n",
    "\t\t\t\tv += self.Compute(p,label[i])\n",
    "\t\t\treturn v\n",
    "\t\treturn (prediction - label)**2\n",
    "\t\n",
    "\tdef Gradient(self,prediction,label):\n",
    "\t\treturn 2 * (prediction - label)\n",
    "\t\n",
    "class Node:\n",
    "\tdef __init__(self):\n",
    "\t\tself.Weights = np.zeros((0,)) \n",
    "\t\t\n",
    "\n",
    "\tdef Initialise(self, dimension):\n",
    "\t\tself.Weights = np.random.normal(0,0.3,(dimension,))\n",
    "\t\tself.InternalGradient = np.zeros(self.Weights.shape)\n",
    "\t\tself.M = np.zeros(self.Weights.shape)\n",
    "\t\tself.V = np.zeros(self.Weights.shape)\n",
    "\t\t\n",
    "\t\tself.L = 1\n",
    "\t\tself.Y = 0\n",
    "\t\tself.Batch = 0\n",
    "\t\tself.dLdY = 0\n",
    "\n",
    "\tdef Predict(self, inputVector): ##I assume that the layer will augment for me, to save some CPU cycles\n",
    "\t\tself.Y = np.dot(self.Weights,inputVector)\n",
    "\t\treturn self.ActivationFunction(self.Y)\n",
    "\t\n",
    "\tdef ActivationFunction(self, inputValue):\n",
    "\t\treturn inputValue\n",
    "\n",
    "\tdef ActivationGradient(self, inputValue):\n",
    "\t\treturn 1\n",
    "\t\n",
    "\tdef UpdateGradient(self, dLdY, inputVector):\n",
    "\t\tif not np.isinf(dLdY):\n",
    "\t\t\tself.dLdY = dLdY\n",
    "\t\t\tself.InternalGradient += self.dLdY * inputVector \n",
    "\t\tself.Batch += 1\n",
    "\tdef UpdateWeights(self,alpha):\n",
    "\n",
    "\n",
    "\t\t## this is the advanced update weight (the ADAM optimiser)\n",
    "\t\tb1 = 0.5\n",
    "\t\tb2 = 0.7\n",
    "\t\tc1 = 1.0/(1.0 - b1**self.L)\n",
    "\t\tc2 = 1.0/(1.0 - b2**self.L)\n",
    "\t\tself.L += 1\n",
    "\t\t\n",
    "\t\tself.InternalGradient/=self.Batch\n",
    "\t\tself.Batch = 0\n",
    "\t\tself.M = b1 * self.M + (1.0 - b1) * self.InternalGradient\n",
    "\t\tself.V = b2 * self.V + (1.0 - b2) * self.InternalGradient**2\n",
    "\n",
    "\t\tself.Weights -= alpha * (c1 *self.M)/np.sqrt(self.V*c2 + 1e-20)\n",
    "\t\tself.InternalGradient *= 0 ## clear for the next iteration!\n",
    "\n",
    "\n",
    "\t\t## this is the default\n",
    "\t\t# step = alpha * self.InternalGradient / np.linalg.norm(self.InternalGradient)\n",
    "\t\t# self.Weights -= step\n",
    "\t\t# self.InternalGradient *= 0 ## clear for the next iteration!\n",
    "\n",
    "class SigmoidNode (Node):\n",
    "\n",
    "\tdef ActivationFunction(self, y):\n",
    "\t\treturn 1.0/(1.0 + np.exp(-y))\n",
    "\tdef ActivationGradient(self, y):\n",
    "\t\treturn np.exp(-y)/(1.0 + np.exp(-y))**2\n",
    "class ReluNode (Node):\n",
    "\n",
    "\tdef ActivationFunction(self, y):\n",
    "\t\tif y > 0:\n",
    "\t\t\treturn y\n",
    "\t\treturn 0.1*y\n",
    "\tdef ActivationGradient(self, y):\n",
    "\t\tif y > 0:\n",
    "\t\t\treturn 1\n",
    "\t\treturn 0.1\n",
    "\n",
    "## A layer is a collection of nodes. The layer takes an input vector, distributes them to the nodes, and then aggregates the data.\n",
    "class Layer:\n",
    "\tdef __init__(self,nodeCount, nodeType=Node):\n",
    "\t\tself.Nodes = []\n",
    "\t\tfor i in range(nodeCount):\n",
    "\t\t\tself.Nodes.append(nodeType())\n",
    "\t\tself.OutputVector = np.zeros((nodeCount+1,))\n",
    "\t\tself.OutputVector[0] = 1\n",
    "\t\t\n",
    "\tdef Initialise(self,previousLayerDimension):\n",
    "\t\tfor node in self.Nodes:\n",
    "\t\t\tnode.Initialise(previousLayerDimension)\n",
    "\n",
    "\tdef Predict(self, inputVector):\n",
    "\t\tfor i,node in enumerate(self.Nodes):\n",
    "\t\t\tself.OutputVector[i+1] = node.Predict(inputVector) ##offset for augmentation purposes!\n",
    "\n",
    "\tdef MiddleLayerGradient(self,layerAbove,inputVector):\n",
    "\t\tfor i,node in enumerate(self.Nodes):\n",
    "\n",
    "\t\t\tdLdy = 0\n",
    "\t\t\tfor innernode in layerAbove.Nodes:\n",
    "\t\t\t\tdLdy += innernode.Weights[i+1] * innernode.dLdY\n",
    "\t\t\tdLdy *= node.ActivationGradient(node.Y)\n",
    "\t\t\tnode.UpdateGradient(dLdy,inputVector)\n",
    "\n",
    "\tdef FinalLayerGradient(self,dLdP,inputVector):\n",
    "\t\tfor node in self.Nodes:\n",
    "\t\t\tnode.UpdateGradient(dLdP * node.ActivationGradient(node.Y),inputVector)\n",
    "\n",
    "\tdef UpdateWeights(self,alpha):\n",
    "\t\tfor node in self.Nodes:\n",
    "\t\t\tnode.UpdateWeights(alpha)\n",
    "\t\t\n",
    "class Network:\n",
    "\tdef __init__(self, inputDimension, outputDimension=1):\n",
    "\t\t#input dimension is the dimension of your data -- so [x,y] is 2. \n",
    "\t\t#output dimension is the dimension of your prediction -- for now this is a binary yes/no -- so one dimension. \n",
    "\n",
    "\t\tself.Layers = [] #this is where your layers will go\n",
    "\t\tself.InputDimension = inputDimension\n",
    "\t\tself.OutputDimension = outputDimension\n",
    "\n",
    "\tdef AddLayer(self, layerObject): #append a layer object into your network. \n",
    "\t\tself.Layers.append(layerObject)\n",
    "\n",
    "\tdef Initialise(self):\n",
    "\t\t##need an initialisation function because your network doesn't know (for instance) what dimension each of the weight vectors should be until it can see the final layer structure -- remember the dimension of the n-th layer weight vectors is equal to the number of nodes in the (n-1)th layer. \n",
    "\n",
    "\n",
    "\t\t## this funciton should initialise each layer, giving them the correct information\n",
    "\t\tpreviousLayerDimension = self.InputDimension\n",
    "\t\tfor layer in self.Layers:\n",
    "\t\t\tlayer.Initialise(previousLayerDimension+1) ## the +1 is for the `bias` term. If you're manually including the bias, you don't need this\n",
    "\t\t\tpreviousLayerDimension = len(layer.Nodes)\n",
    "\t\n",
    "\t\t##then check that my dimensions make sense!\n",
    "\t\tif previousLayerDimension != self.OutputDimension:\n",
    "\t\t\tprint(\"ERROR! Your network output dimension is not equal to the final layer dimension\")\n",
    "\n",
    "\tdef Predict(self, inputVector):\n",
    "\t\tif len(inputVector) == self.InputDimension:\n",
    "\t\t\tinputVector = np.insert(inputVector,0,1)\n",
    "\t\tself.Layers[0].Predict(inputVector)\n",
    "\t\tfor i in range(1,len(self.Layers)):\n",
    "\t\t\tself.Layers[i].Predict(self.Layers[i-1].OutputVector)\n",
    "\t\treturn self.Layers[-1].OutputVector[1:] ##have to omit the augment from the final layer\n",
    "\t\n",
    "\tdef Train(self, data,labels,costFunc = CostFunction(),Nsteps=1000):\n",
    "\t\tself.Initialise()\n",
    "\n",
    "\t\tN = len(data)\n",
    "\t\taugmentedData = []\n",
    "\t\tfor i in range(N):\n",
    "\t\t\taugmentedData.append(np.insert(data[i],0,1)) #augment the data up front\n",
    "\n",
    "\t\talpha = 0.1\n",
    "\t\tids = np.arange(len(data))\n",
    "\t\tfor steps in range(Nsteps):\n",
    "\t\t\tC = 0\t\t\t\n",
    "\t\t\n",
    "\t\t\tfor id in ids:\n",
    "\t\t\t\tp = self.Predict(augmentedData[id])\n",
    "\t\t\t\tC += costFunc.Compute(p,labels[id])\n",
    "\n",
    "\t\t\t\tdCdP = costFunc.Gradient(p,labels[id])\n",
    "\t\t\t\tself.Layers[-1].FinalLayerGradient(dCdP,self.Layers[-2].OutputVector)\n",
    "\t\t\t\tfor j in np.arange(len(self.Layers)-2,-1,-1):\n",
    "\t\t\t\t\tif j > 0:\n",
    "\t\t\t\t\t\tself.Layers[j].MiddleLayerGradient(self.Layers[j+1],self.Layers[j-1].OutputVector)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tself.Layers[j].MiddleLayerGradient(self.Layers[j+1],augmentedData[id])\n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\tfor lNum,layer in enumerate(self.Layers):\n",
    "\t\t\t\tlayer.UpdateWeights(alpha)\n",
    "\t\t\tif steps % 100 == 0:\n",
    "\t\t\t\tprint(steps,\"C=\",C)\n",
    "\t\t\t\talpha*=0.9\t\t\t\t\n",
    "\t\treturn\n",
    "\t\n",
    "\t## this is one of the advanced methods \n",
    "\tdef BatchTrain(self, data,labels,costFunc = CostFunction(),Nsteps=1000):\n",
    "\t\tself.Initialise()\n",
    "\n",
    "\t\talpha = 0.1\n",
    "\t\tprevC = None\n",
    "\t\tmeanC = 0\n",
    "\t\tN = len(data)\n",
    "\t\taugmentedData = []\n",
    "\t\tfor i in range(N):\n",
    "\t\t\taugmentedData.append(np.insert(data[i],0,1)) #augment the data up front\n",
    "\n",
    "\t\tif len(data) > 100:\n",
    "\t\t\tbatchSize = len(data)/100\n",
    "\t\telse:\n",
    "\t\t\tbatchSize = len(data)\n",
    "\t\tids = np.arange(N)\n",
    "\t\tq =0\n",
    "\t\tfor steps in range(Nsteps):\n",
    "\t\t\tnp.random.shuffle(ids)\n",
    "\n",
    "\t\t\tbatch = 0\n",
    "\t\t\tsum = 0\t\t\t\t\t\n",
    "\t\t\tfor i in range(N): \n",
    "\t\t\t\tid = ids[i]\n",
    "\t\t\t\tp = self.Predict(augmentedData[id])\n",
    "\t\t\t\tsum += costFunc.Compute(p,labels[id]) #have to call cost func first to populate node.Y (forward-pass)\n",
    "\t\t\t\tdCdP = costFunc.Gradient(p,labels[id]) #computes gradient with respect to the prediction\n",
    "\t\t\t\tself.Layers[-1].FinalLayerGradient(dCdP,self.Layers[-2].OutputVector)\n",
    "\n",
    "\t\t\t\tfor j in np.arange(len(self.Layers)-2,-1,-1):\n",
    "\t\t\t\t\tif j > 0:\n",
    "\t\t\t\t\t\tself.Layers[j].MiddleLayerGradient(self.Layers[j+1],self.Layers[j-1].OutputVector)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tself.Layers[j].MiddleLayerGradient(self.Layers[j+1],augmentedData[id])\n",
    "\t\t\t\tbatch+=1\n",
    "\t\t\t\tif batch == int(batchSize+1):\n",
    "\t\t\t\t\tbatch = 0\n",
    "\t\t\t\t\tfor layer in self.Layers:\n",
    "\t\t\t\t\t\tlayer.UpdateWeights(alpha)\n",
    "\t\t\tif batch!= 0:\n",
    "\t\t\t\tfor layer in self.Layers:\n",
    "\t\t\t\t\tlayer.UpdateWeights(alpha)\n",
    "\t\t\tmeanC += sum\n",
    "\t\t\tq+=1\n",
    "\t\t\tbatchSize += 1\n",
    "\t\t\tif steps % 10 == 0:\n",
    "\t\t\t\t\n",
    "\t\t\t\tmeanC /= q\n",
    "\t\t\t\tq = 0\n",
    "\t\t\t\tif prevC is not None:\n",
    "\t\t\t\t\tif meanC < prevC:\n",
    "\t\t\t\t\t\tprint(\"Improved at\",steps,\"C=\",meanC,batchSize,alpha)\n",
    "\t\t\t\t\t\talpha = min(0.2,alpha*1.1)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tprint(\"Deproved at\",steps,\"C=\",meanC,batchSize,alpha)\n",
    "\t\t\t\t\t\talpha = max(1e-5,alpha*0.7)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\tprevC = meanC*1.0\n",
    "\t\t\t\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, labels, data = LoadCategoricalData(\"../Data/cuteness.dat\")\n",
    "network = Network(2,1)\n",
    "network.AddLayer(Layer(8,ReluNode))\n",
    "network.AddLayer(Layer(1,SigmoidNode))\n",
    "\n",
    "network.Train(data,labels,CostFunction(),1000)\n",
    "\n",
    "ScanClassifierSpace(network,0,10,0,6,100)\n",
    "pt.clim([-0.2,1.2])\n",
    "PlotData(data,labels,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, labels, data = LoadCategoricalData(\"../Data/cuteness_augmented.dat\")\n",
    "network = Network(2,1)\n",
    "network.AddLayer(Layer(4,SigmoidNode))\n",
    "network.AddLayer(Layer(13,ReluNode))\n",
    "network.AddLayer(Layer(1,SigmoidNode))\n",
    "\n",
    "network.Train(data,labels,ProbCost(),2000)\n",
    "\n",
    "ScanClassifierSpace(network,0,10,0,6,100)\n",
    "pt.clim([-0.2,1.2])\n",
    "PlotData(data,labels,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, labels, data = LoadCategoricalData(\"../Data/complex_classifier.dat\")\n",
    "network = Network(2,1)\n",
    "\n",
    "network.AddLayer(Layer(8,ReluNode))\n",
    "network.AddLayer(Layer(13,SigmoidNode))\n",
    "network.AddLayer(Layer(4,ReluNode))\n",
    "network.AddLayer(Layer(1,SigmoidNode))\n",
    "\n",
    "network.BatchTrain(data,labels,ProbCost(),1000)\n",
    "\n",
    "ScanClassifierSpace(network,-6,6,-6,6,300,3)\n",
    "pt.clim([-0.2,1.2])\n",
    "# PlotData(data,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Need to use this on complex_function as the values are non-integers.\n",
    "def LoadNumericalData(file):\n",
    "\tnames = []\n",
    "\tlabels = []\n",
    "\tdata = []\n",
    "\twith open(file) as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tentries = line.rstrip().split(' ')\n",
    "\t\t\tnames.append(entries[0])\n",
    "\t\t\tlabels.append(float(entries[1]))\n",
    "\t\t\tdata.append(np.array([ float(entries[i]) for i in range(2,len(entries))]))\n",
    "\treturn names, labels, data\n",
    "\n",
    "\n",
    "\n",
    "names, labels, data = LoadNumericalData(\"../Data/complex_function.dat\")\n",
    "network = Network(2,1)\n",
    "network.AddLayer(Layer(8,ReluNode))\n",
    "network.AddLayer(Layer(16,ReluNode))\n",
    "network.AddLayer(Layer(8,ReluNode))\n",
    "network.AddLayer(Layer(13,ReluNode))\n",
    "network.AddLayer(Layer(4,ReluNode))\n",
    "network.AddLayer(Layer(1))\n",
    "network.BatchTrain(data,labels,CostFunction(),1240)\n",
    "\n",
    "\n",
    "ScanClassifierSpace(network,-6,6,-6,6,100,4,False)\n",
    "pt.colorbar()\n",
    "pt.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Cost Functions\n",
    "\n",
    "The following cost function uses logarithms, rather than the least squares. It should make the optimiser converge faster by prioritising 'very wrong' answers over 'slightly wrong'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbCost:\n",
    "\tdef Compute(self,prediction,label):\n",
    "\t\tif label == 1:\n",
    "\t\t\treturn -np.log(prediction+0.00000001) ## give an offset so you never accidentally compute log(0) \n",
    "\t\telse:\n",
    "\t\t\treturn -np.log(1.00000001 - prediction)\n",
    "\t\n",
    "\tdef Gradient(self,prediction,label):\n",
    "\t\tif label == 1:\n",
    "\t\t\treturn -1.0/(prediction+0.00000001)\n",
    "\t\telse:\n",
    "\t\t\treturn 1.0/(1.00000001 - prediction)\n",
    "\t\t\n",
    "names, labels, data = LoadCategoricalData(\"../Data/cuteness_augmented.dat\")\n",
    "network = Network(2,1)\n",
    "network.AddLayer(Layer(8,ReluNode))\n",
    "network.AddLayer(Layer(16,SigmoidNode))\n",
    "network.AddLayer(Layer(1,SigmoidNode))\n",
    "\n",
    "network.Train(data,labels,ProbCost(),1000)\n",
    "\n",
    "ScanClassifierSpace(network,0,10,0,6,100)\n",
    "pt.clim([-0.2,1.2])\n",
    "PlotData(data,labels,names)\n",
    "\n",
    "\n",
    "names, labels, data = LoadCategoricalData(\"../Data/complex_classifier.dat\")\n",
    "network = Network(2,1)\n",
    "network.AddLayer(Layer(8,ReluNode))\n",
    "network.AddLayer(Layer(1,SigmoidNode))\n",
    "\n",
    "network.Train(data,labels,CostFunction(),1000)\n",
    "\n",
    "ScanClassifierSpace(network,-6,6,-6,6,100)\n",
    "pt.clim([-0.2,1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
