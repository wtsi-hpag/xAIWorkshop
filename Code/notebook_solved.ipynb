{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning From First Principles\n",
    "### A Workshop by Jack Fraser-Govil\n",
    "\n",
    "\n",
    "Hello, and welcome to this notebook. Along with the notes, presentation and discussions which we will be having throughout the day, this document will walk you through a series of tasks and exercises which will culminate, by the end of the workshop, in you possessing a fully working Feedforward Neural Network which you have designed and written from scratch. \n",
    "\n",
    "In doing so, we hope that you will gain a degree of insight into how these machines are `thinking' under the hood, and thereby pull back some of the mystery and folklore that surrounds ML, and thereby gain a peek inside the black box. We hope that this knowledge will help inform the discussions and presentations for the rest of this conference. \n",
    "\n",
    "## What Do I Need?\n",
    "\n",
    "In order to run this notebook, all you need is the ability to run an ipython/Jupyter notebooks, and some standard python libraries: mostly ```numpy``` and ```matplotlib```. You do *not* need to have ```tensorflow``` or ```pytorch```: the entire point of this exercise is to create a learning framework without relying on pre-built tools.\n",
    "\n",
    "You *will* need to have read, or be confident that you already know the mathematics contained within [the provided notes](Notes.pdf). If you haven't read them yet (and are not confident that your skillset includes the vector-chain rule and matrix algebra), then I suggest you speed-read Chapters 2, 3 and 4.\n",
    "\n",
    "The final thing you need is a willingness to engage with some of the murkier, more difficult and mathematically complex components of the workshop: teasing out what these things *mean* is why you are here; so if something sticks out to you as something you don't understand, don't just let it wash over you -- please ask and we can discuss. \n",
    "\n",
    "\n",
    "## What if I get stuck?\n",
    "\n",
    "The first port of call is always to ask me, one of my helpers, or simply someone near to you, for guidance. \n",
    "\n",
    "If there's a concept you just can't grasp during the day, I will point you to some resources that you can look at afterwards, and hopefully things will become clearer in retrospect. \n",
    "\n",
    "If you get stuck on one of the coding exercises, or cannot complete it before it is time to move on, you will find a [second notebook in this same directory](notebook_solved.ipynb). This notebook will have all of the coding exercises solved for you. That is not to say that it is 'correct', only that this notebook will be *functional*. If you get really, really, really stuck, you can start stealing code from that document. \n",
    "\n",
    "##### However\n",
    "\n",
    "Whilst it is certainly possible (and permitted) for you to do this at any time, the entire purpose of this workshop is for you to come away with the satisfaction of having written your own learning networks from scratch. We are, by definition, reinventing the wheel here: everything we make will be orders of magnitude worse than pytorch or tensorflow; the principle is that **you learn best by doing it yourself**.\n",
    "\n",
    "If you're going to use the 'cheat sheet', then try, first of all, to simply look at it to understand what it is doing, and then implement your own version. Don't directly copy whole chunks of code you don't understand, as that sort of defeats the point of today's efforts.\n",
    "\n",
    "\n",
    "## Jack, Why is this Python Written Like Garbage?\n",
    "\n",
    "That's a very good and very important question. **I am not a python developer**. As a member of the High Performance Algorithms group, I spend most of my time buried in C++ and C code. I have written this workshop assuming that, since the majority of existing ML products are pythonic, that python would be the most accessible language. There are often times where my python code is written like someone used to C-syntax, and blithely unaware to the beautiful, elegant, pythonic way of doing things.\n",
    "\n",
    "I'm afraid you'll just have to deal with my slightly sub-par python. \n",
    "\n",
    "What was **not** a result of subpar python is the choice not to use ```pandas``` for data. I made a conscious decision to keep things as 'mathematical' as possible, rather than abstracted into dataframes. You get a list of vectors (and maybe some labels), and have to deal with them manually, that's part of the bare-bones experience!\n",
    "\n",
    "I also use ```CamelCase```, not ```snake_case```. Fight me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: The Pretrained Perceptron\n",
    "\n",
    "Just to get the juices flowing, let's get started with an easy project.\n",
    "\n",
    "**In the ```Data``` directory is a file ```cuteness.tex```, which contains (non-dimensional) measures of size and furriness of some animals, as well as some human-curated labels (1 for cute, 0 for note cute). Please write a method which performs Perceptron classification, and use it to identify which of these creatures the model currently mislabels.**  \n",
    "\n",
    "I've provided some basic functions and a class framework for you to work from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are running this in Google Colabs, then you need to uncomment these lines; they act to clone the workshop git locally so that you have access to the data we will be using.\n",
    "# !git clone https://github.com/wtsi-hpag/xAIWorkshop.git\n",
    "# %cd xAIWorkshop/Code\n",
    "\n",
    "# This is the extent of fancy libraries we will be using\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "\n",
    "##load and parse the data manually. No pandas to be seen here! This function will be used repeatedly to load in our test data. \n",
    "def LoadCategoricalData(file):\n",
    "\tnames = []\n",
    "\tlabels = []\n",
    "\tdata = []\n",
    "\twith open(file) as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tentries = line.rstrip().split(' ')\n",
    "\t\t\tnames.append(entries[0])\n",
    "\t\t\tlabels.append(int(entries[1]))\n",
    "\t\t\tdata.append(np.array([ float(entries[i]) for i in range(2,len(entries))]))\n",
    "\treturn names, labels, data\n",
    "\n",
    "names, labels, data = LoadCategoricalData(\"Data/cuteness.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's a basic plotting function, feel free to write your own\n",
    "def PlotData(positions, labels,names=None):\n",
    "\tcols = [\"red\",\"blue\"]\n",
    "\tfor i,pos in enumerate(positions):\n",
    "\t\tc = cols[int(labels[i])]\n",
    "\t\tpt.scatter(pos[0],pos[1],color=c)\n",
    "\t\tif names is not None:\n",
    "\t\t\tpt.annotate(names[i],pos)\n",
    "\tpt.show()\n",
    "\n",
    "pt.title(\"True Assignments\")\n",
    "pt.xlabel(\"Size\")\n",
    "pt.ylabel(\"Fur\")\n",
    "PlotData(data,labels,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.Weights = np.array([1,1,-1]) ## we'll use these pre-provided weights for now\n",
    "\n",
    "\tdef Predict(self,input):\n",
    "\t\t\n",
    "\t\tif type(input) is list:\n",
    "\t\t\tout = []\n",
    "\t\t\tfor r in input:\n",
    "\t\t\t\t# print(r)\n",
    "\t\t\t\tout.append(self.Predict(r))\n",
    "\t\t\treturn out\n",
    "\t\treturn 1.0 * (np.dot(self.Weights,np.insert(input,0,1)) > 0)\n",
    "\n",
    "P = Perceptron()\n",
    "\n",
    "x = np.linspace(0,10,10)\n",
    "w = P.Weights\n",
    "y = -w[1]*x/w[2] - w[0]/w[2]\n",
    "pt.plot(x,y)\n",
    "\n",
    "predict_labels = P.Predict(data)\n",
    "\n",
    "# mislabels = p\n",
    "miss = np.array(names)[predict_labels != np.array(labels)]\n",
    "\n",
    "print(\"Mislabelled animals are:\", miss)\n",
    "print(f\"Success Rate= {round(100*(1.0 - len(miss) * 1.0/len(names)))}%\")\n",
    "\n",
    "PlotData(data,predict_labels==np.array(labels),names)\n",
    "\n",
    "\n",
    "## Initialise, then call your Perceptron on data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Training The Perceptron\n",
    "\n",
    "You should (hopefully) now have a working Perceptron; the only issues is a) it's absolutely terrible and b) I had to give you the weights. \n",
    "\n",
    "The next challenge is to write up a training algorithm for the Perceptron. \n",
    "\n",
    "**Add a method (Train) to the Perceptron, which iterates over the dataset a number of times (you choose - when is 'enough'?) and updates the weights**\n",
    "\n",
    "Remember, given a datum $\\mathbf{x}$, a prediction $P$ and the correct label $L$ the update formula is:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{w} \\to \\mathbf{w} + r \\times \\left(L - P(\\mathbf{x}) \\right)\\mathbf{x}\n",
    "\\end{equation}\n",
    "Where $r$ is your learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I'm fully redefining the Perceptron class; just so that these notebook is iteratively solved. You don't have to do that!\n",
    "class Perceptron:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.Weights = np.array([1.0,1,-1]) ## we'll use these pre-provided weights for now\n",
    "\n",
    "\tdef Predict(self,input):\n",
    "\t\t\n",
    "\t\tif type(input) is list:\n",
    "\t\t\tout = []\n",
    "\t\t\tfor r in input:\n",
    "\t\t\t\t# print(r)\n",
    "\t\t\t\tout.append(self.Predict(r))\n",
    "\t\t\treturn out\n",
    "\t\n",
    "\t\tif len(input) == len(self.Weights):\n",
    "\t\t\treturn 1.0 * (np.dot(self.Weights,input) > 0)\n",
    "\t\telse:\n",
    "\t\t\treturn 1.0 * (np.dot(self.Weights,np.insert(input,0,1)) > 0)\n",
    "\n",
    "\tdef Train(self, data,labels,fullEpochs = 100):\n",
    "\n",
    "\t\trate = 0.2\n",
    "\t\tfor l in range(fullEpochs):\n",
    "\t\t\tnoMistakes = True\n",
    "\t\t\tfor r in range(len(data)):\n",
    "\t\t\t\tx = np.insert(data[r],0,1) ## bulk up with the additional dimension.\n",
    "\t\t\t\tP = self.Predict(x)\n",
    "\n",
    "\t\t\t\tupdate = rate*(labels[r]-P) * x\n",
    "\t\t\t\tself.Weights +=  update\n",
    "\t\t\t\tif (P!= labels[r]):\n",
    "\t\t\t\t\tnoMistakes = False\n",
    "\t\t\tif noMistakes:\n",
    "\t\t\t\tprint(\"Converged early, after\",l,\"epochs\")\n",
    "\t\t\t\treturn\n",
    "P = Perceptron()\n",
    "\n",
    "P.Train(data,labels,50*len(data))\n",
    "\n",
    "## analyse the missed animals\n",
    "predict_labels = P.Predict(data)\n",
    "miss = np.array(names)[predict_labels != np.array(labels)]\n",
    "print(\"Mislabelled animals are:\", miss)\n",
    "print(f\"Success Rate= {round(100*(1.0 - len(miss) * 1.0/len(names)))}%\")\n",
    "\n",
    "## plot a nice line showing the divide\n",
    "x = np.linspace(0,10,10)\n",
    "w = P.Weights\n",
    "y = -w[1]*x/w[2] - w[0]/w[2]\n",
    "pt.plot(x,y)\n",
    "\n",
    "\n",
    "PlotData(data,predict_labels==np.array(labels),names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: The Wrench in the Works\n",
    "\n",
    "If everything has gone to plan, you now have an algorithm that can correctly identify whether a given animal is cute or note, based on measures of their size and amount of fur. \n",
    "\n",
    "Now try running your Perceptron training & testing routines on a new dataset: ```cuteness_augmented```.\n",
    "\n",
    "**What goes wrong?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, labels, data = LoadCategoricalData(\"Data/cuteness_augmented.dat\")\n",
    "P = Perceptron()\n",
    "\n",
    "# get random error\n",
    "predict_labels = P.Predict(data)\n",
    "miss = predict_labels != np.array(labels)\n",
    "failure = np.sum(miss) * 1.0/len(data)\n",
    "s = [0]\n",
    "p = [1.0 - failure]\n",
    "for q in range(2000):\n",
    "\t# print(r)\n",
    "\tP.Train(data,labels,1)\n",
    "\ts.append(len(data)*(q+1))\n",
    "\t\t  \n",
    "\t## analyse the missed animals\n",
    "\tpredict_labels = P.Predict(data)\n",
    "\tmiss = predict_labels != np.array(labels)\n",
    "\tfailure = np.sum(miss) * 1.0/len(data)\n",
    "\tp.append(1.0 - failure)\n",
    "\n",
    "pt.plot(s,p)\n",
    "pt.show()\n",
    "\n",
    "## plot a nice line showing the divide\n",
    "x = np.linspace(0,10,10)\n",
    "w = P.Weights\n",
    "y = -w[1]*x/w[2] - w[0]/w[2]\n",
    "pt.plot(x,y)\n",
    "\n",
    "\n",
    "PlotData(data,predict_labels==np.array(labels),names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Task: The Nonlinear Perceptron\n",
    "\n",
    "It is perfectly possible to generate a Perceptron which can deal with these difficult and non-linear cases: the trick is that you have to put the nonlinearity in yourself. \n",
    "\n",
    "If you have time, try the following:\n",
    "\n",
    "**Write a nonlinear perceptron which transforms the input vector (x,y) into a nonlinear vector (such as (x,y,x^2,y^3 sin(x))), and then uses this as the vector in a standard Perceptron algorithm. Does it show any improvements? How much work does it take to make the algorithm perfectly separate our cute animals from the uggos?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonlinearPerceptron:\n",
    "\n",
    "\tdef __init__(self,maxPower=1):\n",
    "\t\tself.Power = maxPower\n",
    "\t\tself.nDim = int(((maxPower+2) * (maxPower + 1))/2)\n",
    "\t\tself.Weights = np.zeros((self.nDim,)) ## we'll use these pre-provided weights for now\n",
    "\t\tself.Weights[0] = 0.5\n",
    "\t\tself.Weights[1] = 1\n",
    "\t\tself.Weights[2] = -1\n",
    "\n",
    "\t\t# print(self.Weights)\n",
    "\tdef Predict(self,input,vectorfied=False):\n",
    "\t\t\n",
    "\t\tif type(input) is list:\n",
    "\t\t\tout = []\n",
    "\t\t\tfor r in input:\n",
    "\t\t\t\t# print(r)\n",
    "\t\t\t\tout.append(self.Predict(r))\n",
    "\t\t\treturn out\n",
    "\t\tif vectorfied:\n",
    "\t\t\treturn 1.0 * (np.dot(self.Weights,input) > 0)\n",
    "\t\telse:\n",
    "\t\t\tvec = self.Vectorfy(input)\n",
    "\t\t\treturn 1.0 * (np.dot(self.Weights,vec) > 0)\n",
    "\n",
    "\tdef Vectorfy(self,input):\n",
    "\t\t\n",
    "\t\toutput = np.zeros((self.nDim,))\n",
    "\t\toutput[0] = 1\n",
    "\t\ts = 1\n",
    "\t\t\n",
    "\t\tfor power in range(1,self.Power+1):\n",
    "\t\t\tfor q in range(power+1):\n",
    "\t\t\t\toutput[s] = input[0]**q * input[1]**(power-q)\n",
    "\t\t\t\ts+= 1\n",
    "\t\treturn np.array(output)\n",
    "\n",
    "\n",
    "\tdef Train(self, data,labels,fullEpochs = 100):\n",
    "\t\tvectorfied = [self.Vectorfy(d) for d in data]\n",
    "\n",
    "\t\t##this is a bit of a hack (and a hint at better optimisation routines!)\n",
    "\t\t## Because my vector has higher powers of x, the optimiser wobbles incoherently if the powers are x**10 as it's simultaenoulsy trying to balance +/- 1 and +/- a billion\n",
    "\t\t## this slows down the learning at the higher power parameters, letting it be much less sensitive to them\n",
    "\n",
    "\t\trate = 0.1\n",
    "\t\trates = np.zeros((self.nDim))\n",
    "\t\ts = 0\n",
    "\t\tfor power in range(self.Power):\n",
    "\t\t\tfor r in range(power+1):\n",
    "\t\t\t\trates[s] = rate / 10**power\n",
    "\t\t\t\ts+= 1\n",
    "\n",
    "\t\t\t\n",
    "\t\tfor l in range(fullEpochs):\n",
    "\t\t\tnoMistakes = True\n",
    "\t\t\tfor r in range(len(data)):\n",
    "\t\t\t\tP = self.Predict(vectorfied[r],True)\n",
    "\n",
    "\t\t\t\tupdate = rates*(labels[r]-P) * vectorfied[r]\n",
    "\t\t\t\tself.Weights +=  update\n",
    "\t\t\t\tif (P!= labels[r]):\n",
    "\t\t\t\t\tnoMistakes = False\n",
    "\t\t\tif noMistakes:\n",
    "\t\t\t\tprint(\"Converged early, after\",l,\"epochs\")\n",
    "\t\t\t\treturn\n",
    "names, labels, data = LoadCategoricalData(\"Data/cuteness_augmented.dat\")\n",
    "\n",
    "\n",
    "P = NonlinearPerceptron(14)\n",
    "P.Train(data,labels,200000)\n",
    "\n",
    "cols = ['red','blue']\n",
    "x = np.linspace(0,10,140)\n",
    "y = np.linspace(0,6,140)\n",
    "\n",
    "N = len(x)*len(y)\n",
    "xs = np.zeros((N,))\n",
    "ys = np.zeros((N,))\n",
    "zs = ['']*N\n",
    "s= 0\n",
    "z = np.zeros((len(y),len(x)))\n",
    "for i,xp in enumerate(x):\n",
    "\tfor j,yp in enumerate(y):\n",
    "\t\tl = P.Predict(np.array([xp,yp]))\n",
    "\t\txs[s] = xp\n",
    "\t\tys[s] = yp\n",
    "\t\tzs[s] = cols[int(l)]\n",
    "\t\ts+=1\n",
    "\n",
    "pt.scatter(xs,ys,1,zs)\n",
    "PlotData(data,np.array(labels),names)\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network:\n",
    "\n",
    "\tdef __init__(self,inputDimension,outputDimension):\n",
    "\t\tself.Layers = []\n",
    "\t\tself.InputDim = inputDimension\n",
    "\t\tself.OutputDim = outputDimension\n",
    "\tdef AddLayer(self,nNodes,nodeType):\n",
    "\t\tself.Layers.append(Layer(nNodes,nodeType))\n",
    "\n",
    "\tdef Initialise(self):\n",
    "\t\tif len(self.Layers) == 0 or len(self.Layers[-1].Nodes) != self.OutputDim:\n",
    "\t\t\tprint(\"Adding a linear layer for output conformity\")\n",
    "\t\t\tself.AddLayer(self.OutputDim,Node())\n",
    "\n",
    "\t\tprevDim = self.InputDim\n",
    "\t\tfor i in range(len(self.Layers)):\n",
    "\t\t\tself.Layers[i].Initialise(prevDim)\n",
    "\t\t\tprevDim = len(self.Layers[i].Nodes)\n",
    "\tdef Act(self,inputVector):\n",
    "\t\tself.Layers[0].Act(inputVector)\n",
    "\t\tfor l in range(1,len(self.Layers)):\n",
    "\t\t\tself.Layers[l].Act(self.Layers[l-1].Vector)\n",
    "\t\treturn self.Layers[-1].Vector\n",
    "class Layer:\n",
    "\n",
    "\tdef __init__(self,nNodes,nodeType):\n",
    "\n",
    "\t\tself.Nodes = [nodeType]*nNodes\n",
    "\n",
    "\t\tself.Vector = np.zeros(nNodes)\n",
    "\n",
    "\tdef Initialise(self,nDim):\n",
    "\t\tfor i in range(len(self.Nodes)):\n",
    "\t\t\tself.Nodes[i].Initialise(nDim)\n",
    "\n",
    "\tdef Act(self,vector):\n",
    "\t\tfor i in range(len(self.Nodes)):\n",
    "\t\t\tself.Vector[i] = self.Nodes[i].Act(vector)\n",
    "class Node:\n",
    "\n",
    "\tdef __init__(self,func=None):\n",
    "\t\tif func is not None:\n",
    "\t\t\tself.ActivationFunction = func\n",
    "\tdef Initialise(self,nDim):\n",
    "\t\tself.Weights = np.ones((nDim,))*np.random.random()\n",
    "\n",
    "\tdef ActivationFunction(self,x):\n",
    "\t\treturn x\n",
    "\t\n",
    "\tdef Act(self,vector):\n",
    "\t\treturn self.ActivationFunction(np.dot(self.Weights,vector))\n",
    "\t\n",
    "\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network(5,1)\n",
    "network.AddLayer(10,Node())\n",
    "network.Initialise()\n",
    "\n",
    "s=network.Act([5,3,2,1,4])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
